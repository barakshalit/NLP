import java.io.*;
import gnu.trove.*;
/*
 * Adapted from a program by Gregor Heinrich:
 *
 * (C) Copyright 2005, Gregor Heinrich (gregor :: arbylon : net) (This file is
 * part of the org.knowceans experimental software packages.)
 *
 * Also under GPL.
 */

import java.text.DecimalFormat;
import java.text.NumberFormat;

class FileBasedLemmaLexicon implements LemmaLexicon { // {{{

   final int _tokenCount;
   final int _lemmaCount;
   final int[][] _lemForWord; // _lemForWord[wi] --> an array of possible lemma ids for wi 
   final int[] _lwc; // _lwc[li] --> num of possible words for lemma i

   public FileBasedLemmaLexicon(File f) throws IOException {
      BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(f))) ;
      this._lemmaCount = Integer.parseInt(br.readLine());
      this._tokenCount = Integer.parseInt(br.readLine());
      // word counts for lemmas
      this._lwc = new int[this._lemmaCount];
      for (int i=0;i<this._lemmaCount;i++) {
         String line = br.readLine();
         String[] parts = line.split(" ");
         int lemid = Integer.parseInt(parts[0]);
         int wcount = Integer.parseInt(parts[1]);
         this._lwc[lemid] = wcount;
      }
      // possible lemmas for each word
      String line = br.readLine();
      assert (line.length()==0);
      this._lemForWord = new int[this._tokenCount][];
      while ((line = br.readLine()) != null) {
         String[] parts = line.split(" ");
         int tokid = Integer.parseInt(parts[0]);
         this._lemForWord[tokid] = new int[parts.length - 1];
         for (int i=1;i<parts.length;i++) {
            this._lemForWord[tokid][i-1] = Integer.parseInt(parts[i]);
         }
      }
   }

   public int countTokens() { return _tokenCount; }

   public int countLemmas() { return _lemmaCount; }

   public int[] possibleLemmasForWord(int wi) { return _lemForWord[wi]; }

   //public int numWordsForLemma(int li) { return _lwc[li]; }
   public int numWordsForLemma(int li) { return _tokenCount; }

} // }}}

/**
 * Gibbs sampler for estimating the best assignments of topics for words and
 * documents in a corpus. The algorithm is introduced in Tom Griffiths' paper
 * "Gibbs sampling in the generative model of Latent Dirichlet Allocation"
 * (2002).
 * 
 * @author heinrich
 */
public class LemmLdaGibbsSampler {

    String OUT_FILE = "out";
    /**
     * document data (term lists) -- observed data
     */
    int[][] documents;

    /**
     * token vocabulary size (num of tokens) [count words]
     */
    int CW;

    /**
     * lemma vocabulary size (num of possible lemmas) [count lemmas]
     */
    int CL;

    /**
     * number of topics
     */
    int K;

    /**
     * Dirichlet parameter (document--topic associations)
     */
    double alpha;

    /**
     * Dirichlet parameter (topic--lemma associations)
     */
    double beta;

    /**
     * Dirichlet parameter (lemma--term associations)
     */
    double gamma;

    /**
     * topic assignments for each word.
     */
    int z[][];  // DOC:WORD

    /**
     * lemmas assignments for each word
     */
    int l[][];  // DOC:WORD

    /**
     * nlt[i][j] number of instances of lemma i assigned to topic j.
     */
    int[][] nlt;

    /**
     * ndt[i][j] number of tokens/lemmas in document i assigned to topic j.
     */
    int[][] ndt;

    /**
     * nlsum[j] total number of tokens/lemmas assigned to topic j.
     */
    int[] nlsum;

    /**
     * ndsum[i] total number of terms in document i.
     */
    int[] ndsum;

    /**
     * nwsum[k] total number of words assigned to lemma k
     */
    int[] nwsum;

    /**
     * nwl[i].get(j) : number of instances of word i generated by lemma j
     */
    TIntIntHashMap[] nwl;

    /**
     * cumulative statistics of theta
     */
    double[][] thetasum;

    /**
     * cumulative statistics of phi
     */
    double[][] phisum;

    /**
     * size of statistics
     */
    int numstats;

    /**
     * sampling lag (?)
     */
    private static int THIN_INTERVAL = 20;

    /**
     * burn-in period
     */
    private static int BURN_IN = 100;

    /**
     * max iterations
     */
    private static int ITERATIONS = 1000;

    /**
     * sample lag (if -1 only one sample taken)
     */
    private static int SAMPLE_LAG;

    private static int dispcol = 0;

    LemmaLexicon lex;
    /**
     * Initialise the Gibbs sampler with data.
     * 
     * @param V
     *            vocabulary size
     * @param data
     */
    public LemmLdaGibbsSampler(int[][] documents, LemmaLexicon lex) {

        this.documents = documents;
        this.lex = lex;
        this.CW = this.lex.countTokens();
        this.CL = this.lex.countLemmas();
    }

    /**
     * Initialisation: Must start with an assignment of observations to topics ?
     * Many alternatives are possible, I chose to perform random assignments
     * with equal probabilities
     * 
     * @param K
     *            number of topics
     * @return z assignment of topics to words
     */
    public void initialState(int K) { // TODO
        int i;

        int M = documents.length;

        // initialise count variables.
        nlt = new int[this.CL][K]; 
        ndt = new int[M][K];
        nlsum = new int[K]; 
        ndsum = new int[M];
        nwl = new TIntIntHashMap[this.CW]; for (int t=0;t<nwl.length;t++) { nwl[t] = new TIntIntHashMap(); }
        nwsum = new int[this.CL]; 

        // The z_i are initialized to values in [1,K] and
        // the l_i are initialized each to a random value compatible with it's corresponding word
        // to determine the initial state of the Markov chain.

        z = new int[M][];
        l = new int[M][];
        for (int m = 0; m < M; m++) {
            int N = documents[m].length;
            z[m] = new int[N];
            l[m] = new int[N];
            for (int n = 0; n < N; n++) {
                int topic = (int) (Math.random() * K);
                z[m][n] = topic;

                int[] lemmas = this.lex.possibleLemmasForWord(documents[m][n]);
                int lemma = lemmas[(int) (Math.random() * lemmas.length)];
                l[m][n] = lemma;

                // number of instances of lemma k assigned to topic j
                nlt[lemma][topic]++;
                // number of words/tokens/lemmas in document i assigned to topic j.
                ndt[m][topic]++;
                // total number of words/tokens/lemmas assigned to topic j.
                nlsum[topic]++;

                // number of instances of word i generated by lemma k
                nwl[documents[m][n]].adjustOrPutValue(lemma,1,1);
                // total number of words generated by lemma k
                nwsum[lemma]++;

            }
            // total number of words in document i
            ndsum[m] = N;
        }
    }

    /**
     * Main method: Select initial state ? Repeat a large number of times: 1.
     * Select an element 2. Update conditional on other elements. If
     * appropriate, output summary for each run.
     * 
     * @param K
     *            number of topics
     * @param alpha
     *            symmetric prior parameter on document--topic associations
     * @param beta
     *            symmetric prior parameter on topic--lemma associations
     * @param gamma 
     *            symmetric prior parameter on lemma--word associations
     */
    private void gibbs(int K, double alpha, double beta, double gamma) {
        this.K = K;
        this.alpha = alpha;
        this.beta = beta;
        this.gamma = gamma;

        // init sampler statistics
        if (SAMPLE_LAG > 0) {  // TODO
            thetasum = new double[documents.length][K];
            //phisum = new double[K][V];
            numstats = 0;
        }

        // initial state of the Markov chain:
        initialState(K);

        System.out.println("Sampling " + ITERATIONS
            + " iterations with burn-in of " + BURN_IN + " (B/S="
            + THIN_INTERVAL + ").");

        for (int i = 0; i < ITERATIONS; i++) {

            // for all z_i
            for (int m = 0; m < z.length; m++) {
                for (int n = 0; n < z[m].length; n++) {

                    // (z_i = z[m][n])
                    // sample from p(z_i|z_-i, w)
                    int topic = sampleTopicFullConditional(m, n);
                    z[m][n] = topic;
                    int lemma = sampleLemmaFullConditional(m, n);
                    l[m][n] = lemma;

                }
            }
            if ((i % 500) == 0) {
               try {
               System.out.println("writing z/l " + i + ".");
               PrintWriter lout = new PrintWriter(new FileOutputStream(new File(OUT_FILE+".dat.l." + i)));
               writeL(lout);
               lout.close();
               PrintWriter zout = new PrintWriter(new FileOutputStream(new File(OUT_FILE+ ".dat.z." + i)));
               writeZ(zout);
               zout.close();
               System.out.println("done.");
               } catch (IOException e) { e.printStackTrace(); }
            }

            if ((i < BURN_IN) && (i % THIN_INTERVAL == 0)) {
                System.out.print("B");
                dispcol++;
            }
            // display progress
            if ((i > BURN_IN) && (i % THIN_INTERVAL == 0)) {
                System.out.print("S");
                dispcol++;
            }
            // get statistics after burn-in
            if ((i > BURN_IN) && (SAMPLE_LAG > 0) && (i % SAMPLE_LAG == 0)) {
                updateParams();
                System.out.print("|");
                if (i % THIN_INTERVAL != 0)
                    dispcol++;
            }
            if (dispcol >= 100) {
                System.out.println();
                dispcol = 0;
            }
        }
    }

    /**
     * Sample a topic z_i from the full conditional distribution: 
     * p(z_i = j |z_-i, l) = (n_-i,j(l_i) + beta)/(n_-i,j(.) + CL * beta) * (n_-i,j(d_i) + alpha)/(n_-i,.(d_i) + K * alpha)
     * 
     * @param m
     *            document
     * @param n
     *            word
     */
    private int sampleTopicFullConditional(int m, int n) { 

        // remove z_i from the count variables
        int topic = z[m][n];
        int lemma = l[m][n];
        nlt[lemma][topic]--;
        ndt[m][topic]--;
        nlsum[topic]--;
        ndsum[m]--;

        // do multinomial sampling via cumulative method:
        double[] p = new double[K];
        for (int k = 0; k < K; k++) {
            p[k] = (nlt[lemma][k] + beta) / (nlsum[k] + CL * beta)
                * (ndt[m][k] + alpha) / (ndsum[m] + K * alpha);
        }
        // cumulate multinomial parameters
        for (int k = 1; k < p.length; k++) {
            p[k] += p[k - 1];
        }
        // scaled sample because of unnormalised p[]
        double u = Math.random() * p[K - 1];
        for (topic = 0; topic < p.length; topic++) {
            if (u < p[topic])
                break;
        }

        // add newly estimated z_i to count variables
        nlt[lemma][topic]++;
        ndt[m][topic]++;
        nlsum[topic]++;
        ndsum[m]++;

        return topic;
    }

    /**
     * Sample a lemma l_i from the full conditional distribution: 
     * TODO: write the formula in terms of n_..
     * 
     * @param m
     *            document
     * @param n
     *            word
     */
    private int sampleLemmaFullConditional(int m, int n) { 

        // remove l_i from the count variables
        int topic = z[m][n];
        int lemma = l[m][n];
        int word  = documents[m][n];
         
        nlt[lemma][topic]--;
        nlsum[topic]--;
        nwl[word].adjustOrPutValue(lemma,-1,-1);
        nwsum[lemma]--;

        // do multinomial sampling via cumulative method:
        int[] lemmas = this.lex.possibleLemmasForWord(word);
        double[] p = new double[lemmas.length];
        for (int k = 0; k < lemmas.length; k++) {
           int lem = lemmas[k];
           int CLW = this.lex.numWordsForLemma(lem);  // # of words that can be generated by lemma k
            p[k] = (nlt[lem][topic] + beta) / (nlsum[topic] + CL * beta)
                * (nwl[word].get(lem) + gamma) / (nwsum[lem] + CLW * gamma);
        }
        // cumulate multinomial parameters
        for (int k = 1; k < p.length; k++) {
            p[k] += p[k - 1];
        }
        // scaled sample because of unnormalised p[]
        double u = Math.random() * p[p.length - 1];
        int lid;
        for (lid = 0; lid < p.length; lid++) {
            if (u < p[lid])
                break;
        }
        lemma = lemmas[lid];

        // add newly estimated l_i to count variables
        nlt[lemma][topic]++;
        nlsum[topic]++;
        nwl[word].adjustOrPutValue(lemma,+1,+1);
        nwsum[lemma]++;

        return lemma;
    }

    /**
     * Add to the statistics the values of theta and phi for the current state.
     */
    private void updateParams() { // TODO??
       /* 
       for (int m = 0; m < documents.length; m++) {
            for (int k = 0; k < K; k++) {
                thetasum[m][k] += (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
            }
        }
        for (int k = 0; k < K; k++) {
            for (int w = 0; w < V; w++) {
                phisum[k][w] += (nw[w][k] + beta) / (nlsum[k] + V * beta);
            }
        }
        numstats++;
        */
    }

    /**
     * Retrieve estimated document--topic associations. If sample lag > 0 then
     * the mean value of all sampled statistics for theta[][] is taken.
     * 
     * @return theta multinomial mixture of document topics (M x K)
     */
    public double[][] getTheta() { // TODO??
        double[][] theta = new double[documents.length][K];
        /*

        if (SAMPLE_LAG > 0) {
            for (int m = 0; m < documents.length; m++) {
                for (int k = 0; k < K; k++) {
                    theta[m][k] = thetasum[m][k] / numstats;
                }
            }

        } else {
            for (int m = 0; m < documents.length; m++) {
                for (int k = 0; k < K; k++) {
                    theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
                }
            }
        }
        */
        return theta;
    }

    /**
     * Retrieve estimated topic--word associations. If sample lag > 0 then the
     * mean value of all sampled statistics for phi[][] is taken.
     * 
     * @return phi multinomial mixture of topic words (K x V)
     */
    public double[][] getPhi() { // TODO?? 
        /*
        double[][] phi = new double[K][V];
        if (SAMPLE_LAG > 0) {
            for (int k = 0; k < K; k++) {
                for (int w = 0; w < V; w++) {
                    phi[k][w] = phisum[k][w] / numstats;
                }
            }
        } else {
            for (int k = 0; k < K; k++) {
                for (int w = 0; w < V; w++) {
                    phi[k][w] = (nw[w][k] + beta) / (nlsum[k] + V * beta);
                }
            }
        }
        return phi;
        */
       return null;
    }

    public void writeDist(double[][] dist,PrintWriter out) { // TODO??
       for (int i=0;i<dist.length;i++) {
	  out.print(i + " ");
	  for (int j=0;j<dist[i].length;j++) {
	     if (dist[i][j] > 0.0001) {
		out.print(j+":"+dist[i][j]+" ");
	     }
	  }
	  out.println();
       }
    }

    public void writeZ(PrintWriter out) { 
       for(int i=0;i<z.length;i++) {
          for (int j=0; j<z[i].length; j++) {
             out.print(z[i][j] + " ");
          }
          out.println();
       }
    }
    public void writeL(PrintWriter out) { 
       for(int i=0;i<l.length;i++) {
          for (int j=0; j<l[i].length; j++) {
             out.print(l[i][j] + " ");
          }
          out.println();
       }
    }

    /**
     * Print table of multinomial data
     * 
     * @param data
     *            vector of evidence
     * @param fmax
     *            max frequency in display
     * @return the scaled histogram bin values
     */
    public static void hist(double[] data, int fmax) {

        double[] hist = new double[data.length];
        // scale maximum
        double hmax = 0;
        for (int i = 0; i < data.length; i++) {
            hmax = Math.max(data[i], hmax);
        }
        double shrink = fmax / hmax;
        for (int i = 0; i < data.length; i++) {
            hist[i] = shrink * data[i];
        }

        NumberFormat nf = new DecimalFormat("00");
        String scale = "";
        for (int i = 1; i < fmax / 10 + 1; i++) {
            scale += "    .    " + i % 10;
        }

        System.out.println("x" + nf.format(hmax / fmax) + "\t0" + scale);
        for (int i = 0; i < hist.length; i++) {
            System.out.print(i + "\t|");
            for (int j = 0; j < Math.round(hist[i]); j++) {
                if ((j + 1) % 10 == 0)
                    System.out.print("]");
                else
                    System.out.print("|");
            }
            System.out.println();
        }
    }

    /**
     * Configure the gibbs sampler
     * 
     * @param iterations
     *            number of total iterations
     * @param burnIn
     *            number of burn-in iterations
     * @param thinInterval
     *            update statistics interval
     * @param sampleLag
     *            sample interval (-1 for just one sample at the end)
     */
    public void configure(int iterations, int burnIn, int thinInterval,
        int sampleLag) {
        ITERATIONS = iterations;
        BURN_IN = burnIn;
        THIN_INTERVAL = thinInterval;
        SAMPLE_LAG = sampleLag;
    }

    /**
     * Driver with example data.
     * 
     * @param args
     */
    public static void main(String[] args) throws IOException { // TODO??

       String DATA_FILE=null;
       String LEM_LEX_FILE=null;
       String OUT_FILE=null;
       int K=0;
       int ITERATIONS=0;

       // good values alpha = 0.5, beta = .01
       // alpha near 0 -> sparser documents (less topics per document)
       // beta near 0 -> sparser topics (less lemmas per topic) [require more topics]
       // gamma near 0 -> sparser lemmas (less words per lemma)
      /* https://lists.cs.princeton.edu/pipermail/topic-models/2006-September/000002.html
      The alpha parameter determines how dominant a topic is going to be in
      the document. A low value of alpha (i.e. 0.05) will make a topic (or
      two) more predominant in the document whereas a larger value will give
      power to more topics. This is due to the fact that a Dir(.) with a low
      value results in a very spiky multinomial (please someone correct me).

      As for the value of k (the number of topics), a large value will result
      in very 'specific' topics (or picking up just strange combinations of
      terms) whereas a low value will results in too broad topics (the most
      common terms in general being at the top of each topic).
      */       
       double alpha=0; // initialized below..
       double beta=0;  // initialized below..
       double gamma=0; // initialized below..

       try {
          DATA_FILE = args[0];    // .dat
          LEM_LEX_FILE = args[1]; // .lemlex

          OUT_FILE = args.length > 2 ? args[2] : DATA_FILE.split("\\.")[0]; // will get .dat.z and .dat.l suffixes

          K = args.length > 3 ? Integer.parseInt(args[3]) : 100;
          ITERATIONS = args.length > 4 ? Integer.parseInt(args[4]) : 2501;

          alpha = args.length > 5 ? Double.parseDouble(args[5]) : 0.5;
          beta  = args.length > 6 ? Double.parseDouble(args[6]) : 0.1;
          gamma = args.length > 7 ? Double.parseDouble(args[7]) : 0.1;
       } catch (ArrayIndexOutOfBoundsException e) {
          System.out.println("usage: LemmLdaGibbsSampler data.dat data.lemlex out num_topics num_iters alpha beta gamma");
          System.out.println(e);
          System.exit(0);
       }

       System.out.println("Start training. Data file:" + DATA_FILE + " lemma lexicon:" + LEM_LEX_FILE + " output:" + OUT_FILE + " K:" + K + " ITERS:" + ITERATIONS);
       // words in documents
       int[][] documents = null;

       // READ DOCUMENTS
       BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(DATA_FILE))) ;
       int ndocs = Integer.parseInt(br.readLine());
       int V = Integer.parseInt(br.readLine());
       documents = new int[ndocs][];
       for (int i=0;i<ndocs;i++) {
          String line = br.readLine();
          if (line.length()==0) { documents[i]=new int[0]; continue; }
          String[] parts = line.split(" ");
          documents[i] = new int[parts.length];
          for (int j=0;j<parts.length;j++) {
             documents[i][j] = Integer.parseInt(parts[j]);
          }
       }
       System.out.println("Initialized docs");

       // READ LEXICON
       LemmaLexicon lemlex = null;
       try {
          lemlex = new FileBasedLemmaLexicon(new File(LEM_LEX_FILE));
       } catch (Exception e) { e.printStackTrace(); System.exit(-1); }


       // Parameters and setup

       int M = documents.length;



       System.out.println("Lemmas Latent Dirichlet Allocation using Gibbs Sampling.");

       LemmLdaGibbsSampler lda = new LemmLdaGibbsSampler(documents, lemlex);
       lda.OUT_FILE = OUT_FILE;
       lda.configure(ITERATIONS, 200, 20, 10);
       lda.gibbs(K, alpha, beta, gamma);

       double[][] theta = lda.getTheta(); // TODO?
       double[][] phi = lda.getPhi();     // TODO?

       PrintWriter out;

       out = new PrintWriter(new FileOutputStream(new File(OUT_FILE+".dat.z")));
       lda.writeZ(out);
       out.close();
       out = new PrintWriter(new FileOutputStream(new File(OUT_FILE+".dat.l")));
       lda.writeL(out);
       out.close();
    }

    static String[] shades = {"     ", ".    ", ":    ", ":.   ", "::   ",
        "::.  ", ":::  ", ":::. ", ":::: ", "::::.", ":::::"};

    static NumberFormat lnf = new DecimalFormat("00E0");

    /**
     * create a string representation whose gray value appears as an indicator
     * of magnitude, cf. Hinton diagrams in statistics.
     * 
     * @param d
     *            value
     * @param max
     *            maximum value
     * @return
     */
    public static String shadeDouble(double d, double max) {
        int a = (int) Math.floor(d * 10 / max + 0.5);
        if (a > 10 || a < 0) {
            String x = lnf.format(d);
            a = 5 - x.length();
            for (int i = 0; i < a; i++) {
                x += " ";
            }
            return "<" + x + ">";
        }
        return "[" + shades[a] + "]";
    }
}



